training:
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2e-5
  num_epochs: 3
  warmup_steps: 100
  weight_decay: 0.01
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"